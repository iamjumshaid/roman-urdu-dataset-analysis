{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.lines as mlines\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sai kha ya her kisi kay bus ki bat nhi hai lak...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>sahi bt h</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Kya bt hai,</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wah je wah</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Are wha kaya bat hai</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review Sentiment\n",
       "0  Sai kha ya her kisi kay bus ki bat nhi hai lak...  Positive\n",
       "1                                          sahi bt h  Positive\n",
       "2                                        Kya bt hai,  Positive\n",
       "3                                         Wah je wah  Positive\n",
       "4                               Are wha kaya bat hai  Positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"dataset.csv\"\n",
    "dataset = read_csv(url)\n",
    "dataset = dataset.iloc[: , :2]\n",
    "dataset.dropna()\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points:  40458\n",
      "Sentiment\n",
      "Negative    5287\n",
      "Neutral     8929\n",
      "Positive    6013\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points: \",dataset.size)\n",
    "print(dataset.groupby('Sentiment').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '11', '12', '14', '15', '20', '50', '70', 'aa', 'aaj', 'aala', 'aam', 'aap', 'aata', 'aaya', 'ab', 'abdul', 'abdullah', 'abhi', 'abi', 'acha', 'achi', 'acting', 'ada', 'afia', 'afrad', 'afridi', 'agar', 'agr', 'ahmed', 'aik', 'aisa', 'aise', 'aisi', 'aj', 'akram', 'alam', 'ali', 'all', 'allah', 'almi', 'altaf', 'amal', 'ameen', 'america', 'and', 'anwar', 'ap', 'apka', 'apki', 'apko', 'apna', 'apnay', 'apne', 'apni', 'apny', 'app', 'are', 'army', 'arsal', 'as', 'asar', 'assembly', 'ata', 'august', 'aunty', 'aur', 'aurat', 'awam', 'award', 'awaz', 'ay', 'aya', 'aye', 'azam', 'azeem', 'ba', 'baad', 'baar', 'baat', 'bacha', 'bachi', 'bachon', 'bad', 'bal', 'balke', 'ban', 'bana', 'banane', 'banaya', 'band', 'bano', 'bar', 'bara', 'baras', 'bare', 'bari', 'bas', 'bat', 'bata', 'be', 'behtar', 'behtareen', 'benazir', 'beshak', 'best', 'beta', 'beti', 'bhai', 'bhar', 'bharat', 'bharti', 'bhe', 'bhee', 'bhi', 'bhot', 'bht', 'bhutto', 'bi', 'bilkul', 'bohat', 'bohot', 'boht', 'bol', 'british', 'bs', 'bt', 'bta', 'bura', 'bus', 'but', 'career', 'chahiye', 'chahye', 'chal', 'chala', 'chand', 'check', 'cheez', 'chief', 'chor', 'chuke', 'chuki', 'class', 'college', 'company', 'court', 'cricket', 'cup', 'da', 'dafa', 'dain', 'dar', 'daraz', 'daur', 'dauran', 'day', 'de', 'december', 'dekh', 'dekha', 'den', 'dena', 'dene', 'deta', 'dete', 'deya', 'di', 'dia', 'dil', 'din', 'diya', 'diye', 'do', 'dono', 'dor', 'doran', 'dosri', 'dost', 'drama', 'dua', 'duniya', 'dy', 'edhi', 'eee', 'eid', 'ek', 'elawa', 'election', 'end', 'england', 'episode', 'es', 'esa', 'ese', 'faida', 'faisla', 'family', 'farma', 'film', 'films', 'final', 'firing', 'for', 'ga', 'gae', 'gaey', 'gai', 'gay', 'gaya', 'gaye', 'ge', 'geet', 'general', 'ghar', 'gi', 'gia', 'good', 'great', 'gy', 'gya', 'ha', 'haal', 'haan', 'had', 'haha', 'hahah', 'hahaha', 'hahahaha', 'hai', 'hain', 'hakeem', 'hal', 'halat', 'ham', 'hamare', 'hamesha', 'han', 'haq', 'har', 'hashim', 'hasil', 'hath', 'hawale', 'hay', 'hazrat', 'he', 'hee', 'hein', 'hen', 'her', 'hero', 'hesiyat', 'hey', 'hi', 'hidayat', 'hissa', 'hm', 'hn', 'ho', 'hoa', 'hoga', 'hogi', 'hoi', 'hon', 'hona', 'hone', 'hoon', 'hospital', 'hota', 'hote', 'hoti', 'hoty', 'howa', 'howae', 'hoye', 'hu', 'hua', 'hue', 'hui', 'hukumat', 'hum', 'hun', 'hussain', 'huwa', 'huye', 'hy', 'ilawa', 'imran', 'in', 'india', 'inhe', 'inho', 'insan', 'inshallah', 'international', 'iqbal', 'is', 'isi', 'iska', 'iski', 'isko', 'islam', 'iss', 'it', 'itna', 'itni', 'ja', 'jaan', 'jab', 'jae', 'jaga', 'jahan', 'jahangir', 'jahil', 'jail', 'jaise', 'jald', 'jan', 'jana', 'janab', 'janbahaq', 'jane', 'janib', 'jao', 'jari', 'jata', 'jate', 'jati', 'jawab', 'jay', 'jaya', 'jaye', 'jb', 'jeet', 'jese', 'ji', 'jin', 'jis', 'jiya', 'jo', 'judge', 'just', 'justice', 'ka', 'kaam', 'kabhi', 'kae', 'kafi', 'kaha', 'kahan', 'kahani', 'kai', 'kal', 'kam', 'kamal', 'kamiyab', 'kamyab', 'kamyabi', 'kar', 'karachi', 'karain', 'karay', 'kare', 'karen', 'karkardagi', 'karna', 'karne', 'karo', 'karta', 'karte', 'karti', 'kary', 'kay', 'kb', 'kch', 'ke', 'kea', 'keh', 'kehna', 'ker', 'kese', 'key', 'keya', 'kha', 'khair', 'khan', 'khana', 'khandan', 'kharab', 'khas', 'khatam', 'khatoon', 'khawateen', 'khayal', 'khelari', 'khidmat', 'khilaf', 'khud', 'khuda', 'khush', 'ki', 'kia', 'kinza', 'kirdar', 'kis', 'kisi', 'kiya', 'kiye', 'ko', 'koi', 'kon', 'koshish', 'kr', 'kren', 'krna', 'krne', 'krny', 'kro', 'krta', 'krti', 'krty', 'kry', 'kuch', 'ky', 'kya', 'kyun', 'la', 'lag', 'laga', 'lagi', 'lagta', 'lahore', 'lakin', 'lambi', 'lanat', 'last', 'lay', 'le', 'leader', 'league', 'lekin', 'lena', 'leya', 'leye', 'lga', 'li', 'lia', 'liay', 'like', 'likh', 'likha', 'liya', 'liye', 'lo', 'log', 'logo', 'logon', 'lol', 'love', 'ly', 'lye', 'ma', 'maa', 'madad', 'magar', 'mai', 'main', 'malik', 'man', 'mar', 'march', 'mard', 'market', 'martaba', 'masha', 'mashallah', 'masla', 'mat', 'match', 'matches', 'may', 'maza', 'me', 'media', 'mei', 'mein', 'men', 'mera', 'meray', 'mere', 'meri', 'mery', 'miandad', 'mien', 'mil', 'mila', 'mili', 'miss', 'mjhe', 'mn', 'mohammed', 'more', 'mqm', 'mubarak', 'muhammad', 'mujhe', 'mujhy', 'mukhtalif', 'mulk', 'muntakhib', 'murad', 'mushkil', 'muslim', 'mutabiq', 'my', 'na', 'naam', 'nae', 'nahe', 'nahein', 'nahi', 'nahin', 'nai', 'nam', 'name', 'national', 'nawaz', 'nay', 'nazar', 'ne', 'new', 'news', 'next', 'ney', 'nh', 'nhe', 'nhi', 'ni', 'nice', 'nikal', 'no', 'not', 'november', 'number', 'ny', 'october', 'of', 'ok', 'on', 'one', 'open', 'or', 'order', 'our', 'out', 'pa', 'pagal', 'page', 'paida', 'pak', 'pakistan', 'pakistani', 'pani', 'paper', 'par', 'parents', 'parh', 'parha', 'party', 'pas', 'pasand', 'pass', 'pata', 'pay', 'pe', 'pehlay', 'pehle', 'pehli', 'pehly', 'per', 'pesh', 'peshawar', 'phele', 'phir', 'phone', 'phr', 'pic', 'pir', 'please', 'plz', 'police', 'pora', 'pori', 'post', 'pr', 'price', 'product', 'pta', 'pti', 'punjab', 'pur', 'py', 'qabil', 'qaim', 'qarar', 'qaumi', 'quaid', 'quality', 'question', 'raat', 'raha', 'rahay', 'rahe', 'rahi', 'rahy', 'rakh', 'rakha', 'rakhte', 'rakhy', 'ramzan', 'record', 'reh', 'release', 'rha', 'rhi', 'rhy', 'road', 'role', 'roshan', 'roz', 'runs', 'sa', 'saal', 'saath', 'sab', 'sabit', 'sach', 'sadar', 'saeed', 'sahab', 'sahi', 'sahib', 'sakta', 'sakti', 'sal', 'salam', 'salamat', 'salman', 'same', 'samne', 'sari', 'satah', 'sath', 'say', 'sb', 'school', 'se', 'sey', 'shadi', 'shah', 'shaheed', 'shakhs', 'shakhsiyat', 'shamil', 'shareef', 'sharif', 'shayad', 'show', 'shukriya', 'shuru', 'si', 'sindh', 'sir', 'sirf', 'siyasat', 'siyasi', 'size', 'skta', 'so', 'soch', 'song', 'squash', 'sub', 'sun', 'sy', 'ta', 'tadad', 'tag', 'tahum', 'tak', 'taleem', 'tamam', 'taraf', 'tarah', 'tareen', 'tarha', 'taur', 'team', 'tehreek', 'tera', 'tere', 'teri', 'test', 'tha', 'thank', 'thanks', 'that', 'thay', 'the', 'theek', 'thein', 'they', 'thi', 'thin', 'this', 'thy', 'time', 'tk', 'tm', 'to', 'toh', 'too', 'tor', 'tou', 'tu', 'tum', 'tv', 'tw', 'ul', 'umar', 'umer', 'un', 'uncle', 'unhein', 'unhen', 'unho', 'unhon', 'university', 'unki', 'up', 'ur', 'urdu', 'us', 'use', 'uski', 'very', 'video', 'vote', 'wah', 'wahan', 'waheed', 'waja', 'wajah', 'wala', 'walay', 'wale', 'wali', 'walid', 'walo', 'walon', 'waly', 'wapis', 'waqat', 'waqt', 'watan', 'wazir', 'we', 'what', 'with', 'wo', 'woh', 'wohi', 'world', 'xd', 'ya', 'yaad', 'yaar', 'yad', 'yahan', 'yahi', 'yar', 'ye', 'yeh', 'yehi', 'yh', 'you', 'your', 'yr', 'zabardast', 'zakhmi', 'zardari', 'zaroor', 'zia', 'ziada', 'zinda', 'zindagi', 'ziyada', 'ziyadah', 'zyada']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "reviews = list(dataset.Review.astype('U'))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=800)\n",
    "X = vectorizer.fit_transform(reviews).toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, 1] # input labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Models for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algorithm(classifier, X_train, X_test, y_train, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluation Metrix \n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    accuracy_N = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Confusion Matrix:\\n\",confusion_matrix(y_test, y_pred),\"\\n\")\n",
    "\n",
    "    print(\"The accuracy score: \",accuracy_N*100)\n",
    "    f1_score_N = f1_score(y_test, y_pred, average = 'weighted')\n",
    "    print(\"F1 Score: \", f1_score_N*100)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    print(\"The Precision Score: \", precision*100)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    print(\"The Recall Score: \", recall*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Naive Bayes with BOW Vectors and top 800 features\n",
      "\n",
      "20% Testing Data | 80% Training Data\n",
      "Confusion Matrix:\n",
      " [[ 464  474  144]\n",
      " [ 292 1314  224]\n",
      " [ 205  413  516]] \n",
      "\n",
      "The accuracy score:  56.69797330696985\n",
      "F1 Score:  55.96810649912809\n",
      "The Precision Score:  56.2744086188237\n",
      "The Recall Score:  56.69797330696985\n"
     ]
    }
   ],
   "source": [
    "# using Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "print(\"->Naive Bayes with BOW Vectors and top 800 features\\n\")\n",
    "print(\"20% Testing Data | 80% Training Data\")\n",
    "classifier = GaussianNB()\n",
    "run_algorithm(classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Random Forest with BOW Vectors and top 800 features\n",
      "\n",
      "20% Testing Data | 80% Training Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jumsh\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[ 456  413  213]\n",
      " [ 322 1232  276]\n",
      " [ 187  351  596]] \n",
      "\n",
      "The accuracy score:  56.45081562036579\n",
      "F1 Score:  56.09913929824572\n",
      "The Precision Score:  55.95012985221689\n",
      "The Recall Score:  56.45081562036579\n"
     ]
    }
   ],
   "source": [
    "# using Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"->Random Forest with BOW Vectors and top 800 features\\n\")\n",
    "print(\"20% Testing Data | 80% Training Data\")\n",
    "classifier = RandomForestClassifier()\n",
    "run_algorithm(classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->Adboost with BOW Vectors and top 800 features\n",
      "\n",
      "20% Testing Data | 80% Training Data\n",
      "Confusion Matrix:\n",
      " [[ 475  403  204]\n",
      " [ 320 1232  278]\n",
      " [ 203  347  584]] \n",
      "\n",
      "The accuracy score:  56.62382600098863\n",
      "F1 Score:  56.32992418594524\n",
      "The Precision Score:  56.197501288983595\n",
      "The Recall Score:  56.62382600098863\n"
     ]
    }
   ],
   "source": [
    "# using Adboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "print(\"->Adboost with BOW Vectors and top 800 features\\n\")\n",
    "print(\"20% Testing Data | 80% Training Data\")\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "run_algorithm(classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "->KNN with BOW Vectors and top 800 features\n",
      "\n",
      "20% Testing Data | 80% Training Data\n",
      "Confusion Matrix:\n",
      " [[ 470  412  200]\n",
      " [ 353 1186  291]\n",
      " [ 191  354  589]] \n",
      "\n",
      "The accuracy score:  55.48690064260998\n",
      "F1 Score:  55.273254590841844\n",
      "The Precision Score:  55.16173350648997\n",
      "The Recall Score:  55.48690064260998\n"
     ]
    }
   ],
   "source": [
    "#Using KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"->KNN with BOW Vectors and top 800 features\\n\")\n",
    "print(\"20% Testing Data | 80% Training Data\")\n",
    "clf =  KNeighborsClassifier(n_neighbors=3)\n",
    "run_algorithm(classifier, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
